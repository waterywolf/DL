# ---- TENSORFLOW (cleaned) ----
import tensorflow as tf
import numpy as np

# reproducibility (optional)
tf.random.set_seed(42)
np.random.seed(42)

# Create data (same as your original: x ~ random, y = 0.2*x + 0.2)
x_np = np.random.rand(100).astype(np.float32)
y_np = 0.2 * x_np + 0.2

# Convert to tf.Tensor with explicit dtype
x = tf.constant(x_np, dtype=tf.float32)
y = tf.constant(y_np, dtype=tf.float32)

# Model parameters: use scalar variables (shape=())
W = tf.Variable(tf.random.normal(shape=(), dtype=tf.float32), name="W")
bias = tf.Variable(0.0, dtype=tf.float32, name="bias")

optimizer = tf.keras.optimizers.Adam(learning_rate=0.05)

def mse_loss(W, b, x, y):
    y_pred = W * x + b
    return tf.reduce_mean(tf.square(y_pred - y))

# Training loop using GradientTape
for step in range(5000):
    with tf.GradientTape() as tape:
        loss = mse_loss(W, bias, x, y)
    grads = tape.gradient(loss, [W, bias])
    optimizer.apply_gradients(zip(grads, [W, bias]))

    if step % 500 == 0:
        print(f"step {step}: W={W.numpy():.6f}, bias={bias.numpy():.6f}, loss={loss.numpy():.6f}")

# final values
print("Trained:", "W =", W.numpy(), "bias =", bias.numpy())
# expected near 0.2 and 0.2



# ---- PYTORCH (cleaned) ----
import torch
import numpy as np

# reproducibility (optional)
torch.manual_seed(42)
np.random.seed(42)

# simple data example
data = [[1, 2], [3, 4]]
x_data = torch.tensor(data)           # dtype inferred (probably int64)
print("x_data dtype:", x_data.dtype)

# Convert numpy -> torch (explicit dtype conversion)
np_array = np.array(data, dtype=np.float32)
x_from_numpy = torch.from_numpy(np_array)   # float32
print("x_from_numpy:", x_from_numpy)
print("dtype:", x_from_numpy.dtype)

# ones_like, rand_like examples
x_ones = torch.ones_like(x_from_numpy)
x_rand = torch.rand_like(x_from_numpy)
print("ones_like:\n", x_ones)
print("rand_like:\n", x_rand)

# device check and move to GPU if available
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
tensor = torch.rand(3, 4, device=device)
print("tensor device:", tensor.device)

# concatenation example
t1 = torch.ones(4, 4)
t0 = torch.zeros(4, 4)
t_cat = torch.cat([t1, t0], dim=0)  # shape (8,4)
print("cat shape:", t_cat.shape)

# in-place add example (caution: in-place modifies tensor)
t = torch.ones(4, 4)
t.add_(3)   # now all elements are 4
print(t)

# convert torch -> numpy (only allowed for CPU tensors)
cpu_t = t.cpu()
n = cpu_t.numpy()
print("numpy:", n, type(n))
