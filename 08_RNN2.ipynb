{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c2ad1e5-a571-49cd-bddc-6a66fc54414d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vanilla RNN from scratch using NumPy\n",
    "# - character-level next-step prediction example helpers included\n",
    "# - forward, loss, backward (BPTT), parameter update (SGD with grad clipping)\n",
    "# Paste this whole cell into Jupyter.\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class VanillaRNN:\n",
    "    def __init__(self, input_size, hidden_size, output_size, seq_len=25, learning_rate=1e-1, clip=5.0, seed=42):\n",
    "        \"\"\"\n",
    "        input_size : size of one-hot input (vocab size for char-RNN)\n",
    "        hidden_size: number of hidden units\n",
    "        output_size: size of output (same as input_size for next-char prediction)\n",
    "        seq_len    : truncated BPTT length\n",
    "        learning_rate: SGD step size\n",
    "        clip       : gradient clipping threshold (L2)\n",
    "        \"\"\"\n",
    "        np.random.seed(seed)\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.seq_len = seq_len\n",
    "        self.lr = learning_rate\n",
    "        self.clip = clip\n",
    "\n",
    "        # Initialize weights (small random numbers)\n",
    "        self.Wxh = np.random.randn(hidden_size, input_size) * 0.01   # input -> hidden\n",
    "        self.Whh = np.random.randn(hidden_size, hidden_size) * 0.01  # hidden -> hidden\n",
    "        self.Why = np.random.randn(output_size, hidden_size) * 0.01  # hidden -> output\n",
    "        self.bh = np.zeros((hidden_size, 1))\n",
    "        self.by = np.zeros((output_size, 1))\n",
    "\n",
    "        # For simple SGD we don't need momentum or RMSProp here; you can add later.\n",
    "\n",
    "    @staticmethod\n",
    "    def softmax(x):\n",
    "        e = np.exp(x - np.max(x, axis=0, keepdims=True))\n",
    "        return e / np.sum(e, axis=0, keepdims=True)\n",
    "\n",
    "    def forward(self, inputs, hprev):\n",
    "        \"\"\"\n",
    "        inputs: list of integers (one-hot indices) length T\n",
    "        hprev: initial hidden column vector shape (hidden_size, 1)\n",
    "        Returns: loss, activations, last hidden\n",
    "        \"\"\"\n",
    "        xs, hs, ys, ps = {}, {}, {}, {}\n",
    "        hs[-1] = np.copy(hprev)\n",
    "        loss = 0\n",
    "        # forward pass\n",
    "        for t, ix in enumerate(inputs):\n",
    "            x = np.zeros((self.input_size, 1))\n",
    "            x[ix] = 1\n",
    "            xs[t] = x\n",
    "            hs[t] = np.tanh(self.Wxh.dot(x) + self.Whh.dot(hs[t-1]) + self.bh)\n",
    "            ys[t] = self.Why.dot(hs[t]) + self.by\n",
    "            ps[t] = self.softmax(ys[t])\n",
    "        return xs, hs, ys, ps\n",
    "\n",
    "    def compute_loss(self, ps, targets):\n",
    "        \"\"\"\n",
    "        ps: predicted probs dict t->(vocab,1)\n",
    "        targets: list of integer targets, same length as ps\n",
    "        returns: scalar cross-entropy loss\n",
    "        \"\"\"\n",
    "        loss = 0\n",
    "        for t in range(len(targets)):\n",
    "            loss += -np.log(ps[t][targets[t], 0] + 1e-12)\n",
    "        return loss\n",
    "\n",
    "    def backward(self, xs, hs, ps, targets):\n",
    "        \"\"\"\n",
    "        Backprop through time (truncated to len(targets))\n",
    "        Returns gradients for all parameters and last hidden state gradient\n",
    "        \"\"\"\n",
    "        # initialize gradients\n",
    "        dWxh = np.zeros_like(self.Wxh)\n",
    "        dWhh = np.zeros_like(self.Whh)\n",
    "        dWhy = np.zeros_like(self.Why)\n",
    "        dbh = np.zeros_like(self.bh)\n",
    "        dby = np.zeros_like(self.by)\n",
    "        dhnext = np.zeros((self.hidden_size, 1))\n",
    "\n",
    "        T = len(targets)\n",
    "        for t in reversed(range(T)):\n",
    "            dy = np.copy(ps[t])\n",
    "            dy[targets[t]] -= 1  # softmax derivative (cross-entropy)\n",
    "            dWhy += dy.dot(hs[t].T)\n",
    "            dby += dy\n",
    "\n",
    "            dh = self.Why.T.dot(dy) + dhnext  # backprop into h\n",
    "            # backprop through tanh nonlinearity\n",
    "            dhraw = (1 - hs[t] * hs[t]) * dh\n",
    "            dbh += dhraw\n",
    "            dWxh += dhraw.dot(xs[t].T)\n",
    "            dWhh += dhraw.dot(hs[t-1].T)\n",
    "            dhnext = self.Whh.T.dot(dhraw)\n",
    "\n",
    "        # clip gradients to avoid exploding gradients\n",
    "        for grad in (dWxh, dWhh, dWhy, dbh, dby):\n",
    "            np.clip(grad, -self.clip, self.clip, out=grad)\n",
    "\n",
    "        return {'Wxh': dWxh, 'Whh': dWhh, 'Why': dWhy, 'bh': dbh, 'by': dby}, hs[T-1]\n",
    "\n",
    "    def update_params(self, grads):\n",
    "        # Simple SGD update\n",
    "        self.Wxh -= self.lr * grads['Wxh']\n",
    "        self.Whh -= self.lr * grads['Whh']\n",
    "        self.Why -= self.lr * grads['Why']\n",
    "        self.bh  -= self.lr * grads['bh']\n",
    "        self.by  -= self.lr * grads['by']\n",
    "\n",
    "    def sample(self, seed_ix, n, h=None):\n",
    "        \"\"\"\n",
    "        Sample a sequence of length n, given a seed index seed_ix and start hidden state h.\n",
    "        Returns list of sampled indices.\n",
    "        \"\"\"\n",
    "        if h is None:\n",
    "            h = np.zeros((self.hidden_size, 1))\n",
    "        x = np.zeros((self.input_size, 1))\n",
    "        x[seed_ix] = 1\n",
    "        ixes = []\n",
    "        for t in range(n):\n",
    "            h = np.tanh(self.Wxh.dot(x) + self.Whh.dot(h) + self.bh)\n",
    "            y = self.Why.dot(h) + self.by\n",
    "            p = self.softmax(y)\n",
    "            # sample from p\n",
    "            ix = np.random.choice(range(self.input_size), p=p.ravel())\n",
    "            x = np.zeros((self.input_size, 1))\n",
    "            x[ix] = 1\n",
    "            ixes.append(ix)\n",
    "        return ixes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9384b037-cab3-449a-8570-922104f1ceb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 9\n",
      "Chars: ['\\n', ' ', 'd', 'e', 'h', 'l', 'o', 'r', 'w']\n",
      "Iter 1, loss: 54.9306\n",
      "----\n",
      " whd \n",
      "lldoodeweohoolrwldeol \n",
      "odhol\n",
      " ddlh \n",
      " whrrho o \n",
      "----\n",
      "Iter 300, loss: 130.3570\n",
      "----\n",
      " o dlo dlo dlo dlo dlo dlo dlo dlo dlo dlo dlo dlo  \n",
      "----\n",
      "Iter 600, loss: 186.6912\n",
      "----\n",
      " \n",
      "o w\n",
      "o w\n",
      "o w\n",
      "o w\n",
      "o w\n",
      "o w\n",
      "o w\n",
      "o w\n",
      "o w\n",
      "o w\n",
      "o w\n",
      "o w\n",
      "o \n",
      "----\n",
      "Iter 900, loss: 227.1397\n",
      "----\n",
      " lloelloelloelloelloelloelloelloelloelloelloelloell \n",
      "----\n",
      "Iter 1200, loss: 258.6012\n",
      "----\n",
      " elloelloelloelloelloelloelloelloelloelloelloelloel \n",
      "----\n",
      "Iter 1500, loss: 281.1189\n",
      "----\n",
      " rellrel\n",
      "rel\n",
      "rel\n",
      "rel\n",
      "rellrel\n",
      "rel\n",
      "rellrel\n",
      "rel\n",
      "rel\n",
      "re \n",
      "----\n",
      "Iter 1800, loss: 298.4631\n",
      "----\n",
      " \n",
      "h d\n",
      "h d\n",
      "h dlh d\n",
      "h d\n",
      "h d\n",
      "h d\n",
      "h d\n",
      "h d\n",
      "h dlh d\n",
      "h d\n",
      "h \n",
      "----\n",
      "Iter 2100, loss: 310.1957\n",
      "----\n",
      " orlworlworlworlworlworlworlworlworlworlworlworlwor \n",
      "----\n",
      "Iter 2400, loss: 319.8803\n",
      "----\n",
      " wor wor wor wor wor wor wor wor wor wor wor wor wo \n",
      "----\n",
      "Iter 2700, loss: 328.0183\n",
      "----\n",
      "  dor dor dor dor dor dor dor dor dor dor dor dor d \n",
      "----\n",
      "Iter 3000, loss: 331.4525\n",
      "----\n",
      " r w\n",
      "r w\n",
      "r w\n",
      "r w\n",
      "r w\n",
      "r w\n",
      "r w\n",
      "r w\n",
      "r w\n",
      "r w\n",
      "r w\n",
      "r l\n",
      "r  \n",
      "----\n",
      "Final sample:\n",
      " el\n",
      "r w\n",
      "r w\n",
      "r l\n",
      "r w\n",
      "r w\n",
      "r w\n",
      "r w\n",
      "r l\n",
      "r w\n",
      "r w\n",
      "r w\n",
      "r w\n",
      "r w\n",
      "r w\n",
      "r w\n",
      "r w\n",
      "r l\n",
      "r w\n",
      "r w\n",
      "r w\n",
      "r w\n",
      "r w\n",
      "r w\n",
      "r wlr\n"
     ]
    }
   ],
   "source": [
    "# Training example: learn to predict next character for a very small corpus\n",
    "# Paste as a new cell below the previous one and run both.\n",
    "\n",
    "data = \"hello world\\n\" * 200  # small corpus repeated to give enough steps\n",
    "chars = sorted(list(set(data)))\n",
    "vocab_size = len(chars)\n",
    "char_to_ix = {ch: i for i, ch in enumerate(chars)}\n",
    "ix_to_char = {i: ch for ch, i in char_to_ix.items()}\n",
    "\n",
    "print(\"Vocab size:\", vocab_size)\n",
    "print(\"Chars:\", chars)\n",
    "\n",
    "# Hyperparameters\n",
    "hidden_size = 100\n",
    "seq_len = 25\n",
    "learning_rate = 1e-1\n",
    "iterations = 3000\n",
    "print_every = 300\n",
    "\n",
    "# Initialize model\n",
    "rnn = VanillaRNN(input_size=vocab_size, hidden_size=hidden_size, output_size=vocab_size,\n",
    "                 seq_len=seq_len, learning_rate=learning_rate)\n",
    "\n",
    "# Prepare data as list of indices\n",
    "data_ix = [char_to_ix[ch] for ch in data]\n",
    "data_len = len(data_ix)\n",
    "\n",
    "# Training loop (simple SGD over sequence chunks)\n",
    "p = 0  # data pointer\n",
    "smooth_loss = -np.log(1.0 / vocab_size) * seq_len\n",
    "\n",
    "for it in range(1, iterations + 1):\n",
    "    # prepare inputs and targets\n",
    "    if p + seq_len + 1 >= data_len or it == 1:\n",
    "        hprev = np.zeros((hidden_size, 1))  # reset RNN memory\n",
    "        p = 0  # go from start of data\n",
    "    inputs = data_ix[p:p + seq_len]\n",
    "    targets = data_ix[p + 1:p + seq_len + 1]\n",
    "\n",
    "    # forward\n",
    "    xs, hs, ys, ps = rnn.forward(inputs, hprev)\n",
    "    loss = rnn.compute_loss(ps, targets)\n",
    "    smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "\n",
    "    # backward\n",
    "    grads, hprev = rnn.backward(xs, hs, ps, targets)\n",
    "    rnn.update_params(grads)\n",
    "\n",
    "    p += seq_len  # move data pointer\n",
    "\n",
    "    if it % print_every == 0 or it == 1:\n",
    "        print(f\"Iter {it}, loss: {smooth_loss:.4f}\")\n",
    "        # sample from the model\n",
    "        sample_ix = rnn.sample(seed_ix=inputs[0], n=50, h=hs[seq_len-1])\n",
    "        txt = ''.join(ix_to_char[ix] for ix in sample_ix)\n",
    "        print(\"----\\n\", txt, \"\\n----\")\n",
    "\n",
    "# Final sample after training\n",
    "seed = char_to_ix['h']\n",
    "sample_ix = rnn.sample(seed_ix=seed, n=100)\n",
    "print(\"Final sample:\\n\", ''.join(ix_to_char[ix] for ix in sample_ix))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74cc8cc6-d538-4070-b29b-2b6d4921564b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
